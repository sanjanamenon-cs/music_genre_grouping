{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽµ Music Genre Clustering - GMM Implementation\n",
    "\n",
    "## Gaussian Mixture Model - Probabilistic Clustering\n",
    "\n",
    "**Project:** Music Genre Clustering using GMM  \n",
    "**Dataset:** GTZAN (1,000 songs, 10 genres)  \n",
    "**Author:** Vedant  \n",
    "**Date:** October 2025\n",
    "\n",
    "---\n",
    "\n",
    "## Notebook Overview\n",
    "\n",
    "This notebook covers:\n",
    "1. GMM mathematical foundations\n",
    "2. Expectation-Maximization (EM) algorithm\n",
    "3. GMM training and soft clustering\n",
    "4. Probability assignments vs hard labels\n",
    "5. Comparison with K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Mathematical Foundation of GMM\n",
    "\n",
    "### 1.1 Gaussian Mixture Model Definition\n",
    "\n",
    "GMM represents data as a mixture of K Gaussian distributions:\n",
    "\n",
    "$$p(x) = \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(x | \\mu_k, \\Sigma_k)$$\n",
    "\n",
    "Where:\n",
    "- $\\pi_k$ = Mixing coefficient (weight) of component $k$\n",
    "- $\\sum_{k=1}^{K} \\pi_k = 1$\n",
    "- $\\mathcal{N}(x | \\mu_k, \\Sigma_k)$ = Multivariate Gaussian distribution\n",
    "\n",
    "---\n",
    "\n",
    "### 1.2 Multivariate Gaussian Distribution\n",
    "\n",
    "$$\\mathcal{N}(x | \\mu, \\Sigma) = \\frac{1}{(2\\pi)^{d/2} |\\Sigma|^{1/2}} \\exp\\left(-\\frac{1}{2}(x-\\mu)^T \\Sigma^{-1} (x-\\mu)\\right)$$\n",
    "\n",
    "Where:\n",
    "- $d$ = dimensionality (5 features)\n",
    "- $\\mu$ = mean vector\n",
    "- $\\Sigma$ = covariance matrix\n",
    "- $|\\Sigma|$ = determinant of covariance matrix\n",
    "\n",
    "---\n",
    "\n",
    "### 1.3 Expectation-Maximization (EM) Algorithm\n",
    "\n",
    "#### **E-Step (Expectation)**\n",
    "\n",
    "Calculate responsibility of component $k$ for data point $n$:\n",
    "\n",
    "$$\\gamma(z_{nk}) = \\frac{\\pi_k \\mathcal{N}(x_n | \\mu_k, \\Sigma_k)}{\\sum_{j=1}^{K} \\pi_j \\mathcal{N}(x_n | \\mu_j, \\Sigma_j)}$$\n",
    "\n",
    "**Interpretation:** \n",
    "- $\\gamma(z_{nk})$ = Probability that point $n$ belongs to component $k$\n",
    "- Sum over all $k$ equals 1: $\\sum_{k=1}^{K} \\gamma(z_{nk}) = 1$\n",
    "\n",
    "#### **M-Step (Maximization)**\n",
    "\n",
    "Update parameters using weighted MLE:\n",
    "\n",
    "**Effective number of points in component $k$:**\n",
    "$$N_k = \\sum_{n=1}^{N} \\gamma(z_{nk})$$\n",
    "\n",
    "**New means:**\n",
    "$$\\mu_k^{new} = \\frac{1}{N_k} \\sum_{n=1}^{N} \\gamma(z_{nk}) x_n$$\n",
    "\n",
    "**New covariances:**\n",
    "$$\\Sigma_k^{new} = \\frac{1}{N_k} \\sum_{n=1}^{N} \\gamma(z_{nk}) (x_n - \\mu_k^{new})(x_n - \\mu_k^{new})^T$$\n",
    "\n",
    "**New mixing coefficients:**\n",
    "$$\\pi_k^{new} = \\frac{N_k}{N}$$\n",
    "\n",
    "---\n",
    "\n",
    "### 1.4 GMM vs K-Means\n",
    "\n",
    "| Aspect | K-Means | GMM |\n",
    "|--------|---------|-----|\n",
    "| Clustering | Hard (binary) | Soft (probabilistic) |\n",
    "| Assignment | $x \\in C_k$ | $P(z_k|x) = \\gamma(z_k)$ |\n",
    "| Cluster Shape | Spherical | Elliptical |\n",
    "| Distance Metric | Euclidean | Mahalanobis |\n",
    "| Covariance | Shared (implicit) | Per-component $\\Sigma_k$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Clustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "# Utilities\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"âœ… Libraries loaded successfully!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data\n",
    "features_df = pd.read_csv('data/processed/features_selected.csv')\n",
    "feature_cols = ['tempo', 'energy', 'loudness', 'valence', 'danceability']\n",
    "\n",
    "# Load standardized data\n",
    "X = features_df[feature_cols].values\n",
    "scaler = joblib.load('models/scaler.pkl')\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATASET LOADED\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Shape: {X_scaled.shape}\")\n",
    "print(f\"Features: {feature_cols}\")\n",
    "print(f\"Standardized: âœ…\")\n",
    "print(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Selection - BIC and AIC\n",
    "\n",
    "### Information Criteria\n",
    "\n",
    "**Bayesian Information Criterion (BIC):**\n",
    "$$\\text{BIC} = -2 \\ln(\\mathcal{L}) + p \\ln(N)$$\n",
    "\n",
    "**Akaike Information Criterion (AIC):**\n",
    "$$\\text{AIC} = -2 \\ln(\\mathcal{L}) + 2p$$\n",
    "\n",
    "Where:\n",
    "- $\\mathcal{L}$ = Maximum likelihood\n",
    "- $p$ = Number of parameters\n",
    "- $N$ = Number of data points\n",
    "\n",
    "**Lower is better** - balances fit quality and model complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different numbers of components\n",
    "n_components_range = range(2, 21)\n",
    "bic_scores = []\n",
    "aic_scores = []\n",
    "log_likelihoods = []\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL SELECTION - TESTING DIFFERENT COMPONENTS\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "for n_components in n_components_range:\n",
    "    gmm = GaussianMixture(\n",
    "        n_components=n_components,\n",
    "        covariance_type='full',\n",
    "        max_iter=300,\n",
    "        random_state=42\n",
    "    )\n",
    "    gmm.fit(X_scaled)\n",
    "    \n",
    "    bic = gmm.bic(X_scaled)\n",
    "    aic = gmm.aic(X_scaled)\n",
    "    log_likelihood = gmm.score(X_scaled) * X_scaled.shape[0]\n",
    "    \n",
    "    bic_scores.append(bic)\n",
    "    aic_scores.append(aic)\n",
    "    log_likelihoods.append(log_likelihood)\n",
    "    \n",
    "    print(f\"K={n_components:2d} | BIC: {bic:8.2f} | AIC: {aic:8.2f} | Log-Likelihood: {log_likelihood:8.2f}\")\n",
    "\n",
    "print(\"\\nâœ… Model selection complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot BIC and AIC\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# BIC plot\n",
    "ax1.plot(n_components_range, bic_scores, 'bo-', linewidth=2, markersize=8, label='BIC')\n",
    "ax1.axvline(x=10, color='red', linestyle='--', linewidth=2, label='Chosen K=10')\n",
    "ax1.set_xlabel('Number of Components', fontsize=12)\n",
    "ax1.set_ylabel('BIC Score', fontsize=12)\n",
    "ax1.set_title('Bayesian Information Criterion', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend(fontsize=10)\n",
    "\n",
    "# AIC plot\n",
    "ax2.plot(n_components_range, aic_scores, 'go-', linewidth=2, markersize=8, label='AIC')\n",
    "ax2.axvline(x=10, color='red', linestyle='--', linewidth=2, label='Chosen K=10')\n",
    "ax2.set_xlabel('Number of Components', fontsize=12)\n",
    "ax2.set_ylabel('AIC Score', fontsize=12)\n",
    "ax2.set_title('Akaike Information Criterion', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend(fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find optimal K\n",
    "optimal_k_bic = n_components_range[np.argmin(bic_scores)]\n",
    "optimal_k_aic = n_components_range[np.argmin(aic_scores)]\n",
    "\n",
    "print(f\"\\nOptimal K (BIC): {optimal_k_bic}\")\n",
    "print(f\"Optimal K (AIC): {optimal_k_aic}\")\n",
    "print(f\"Chosen K: 10 (matches dataset structure)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train GMM with K=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train GMM\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING GAUSSIAN MIXTURE MODEL\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "gmm = GaussianMixture(\n",
    "    n_components=10,\n",
    "    covariance_type='full',\n",
    "    max_iter=300,\n",
    "    n_init=10,\n",
    "    random_state=42,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "gmm.fit(X_scaled)\n",
    "\n",
    "print(f\"âœ… Training complete!\")\n",
    "print(f\"\\nModel Parameters:\")\n",
    "print(f\"  n_components: {gmm.n_components}\")\n",
    "print(f\"  covariance_type: {gmm.covariance_type}\")\n",
    "print(f\"  n_iter: {gmm.n_iter_} (actual EM iterations)\")\n",
    "print(f\"  converged: {gmm.converged_}\")\n",
    "print(f\"\\nModel Quality:\")\n",
    "print(f\"  Log-Likelihood: {gmm.score(X_scaled) * X_scaled.shape[0]:.2f}\")\n",
    "print(f\"  BIC: {gmm.bic(X_scaled):.2f}\")\n",
    "print(f\"  AIC: {gmm.aic(X_scaled):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Soft vs Hard Clustering\n",
    "\n",
    "### Probability Assignments\n",
    "\n",
    "GMM provides **soft assignments** (probabilities):\n",
    "\n",
    "For each song $x_n$:\n",
    "$$P(z_k | x_n) = \\gamma(z_{nk}) = \\frac{\\pi_k \\mathcal{N}(x_n | \\mu_k, \\Sigma_k)}{\\sum_{j=1}^{K} \\pi_j \\mathcal{N}(x_n | \\mu_j, \\Sigma_j)}$$\n",
    "\n",
    "**Hard assignment** (for comparison with K-Means):\n",
    "$$\\text{cluster}(x_n) = \\arg\\max_k P(z_k | x_n)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get soft assignments (probabilities)\n",
    "probabilities = gmm.predict_proba(X_scaled)\n",
    "\n",
    "# Get hard assignments\n",
    "hard_labels = gmm.predict(X_scaled)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CLUSTERING ASSIGNMENTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nProbability matrix shape: {probabilities.shape}\")\n",
    "print(f\"Hard labels shape: {hard_labels.shape}\")\n",
    "\n",
    "# Example: Show probabilities for first 5 songs\n",
    "print(\"\\nExample - First 5 Songs:\")\n",
    "print(\"=\"*70)\n",
    "prob_df = pd.DataFrame(\n",
    "    probabilities[:5],\n",
    "    columns=[f'P(C{i})' for i in range(10)],\n",
    "    index=[f'Song {i}' for i in range(5)]\n",
    ")\n",
    "print(prob_df)\n",
    "\n",
    "print(\"\\nHard assignments for first 5 songs:\")\n",
    "print(hard_labels[:5])\n",
    "\n",
    "# Add to dataframe\n",
    "features_df['gmm_cluster'] = hard_labels\n",
    "for i in range(10):\n",
    "    features_df[f'gmm_prob_{i}'] = probabilities[:, i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncertainty Analysis\n",
    "\n",
    "**Entropy of probability distribution:**\n",
    "$$H(p) = -\\sum_{k=1}^{K} p_k \\log p_k$$\n",
    "\n",
    "- **Low entropy** â†’ Confident assignment (one high probability)\n",
    "- **High entropy** â†’ Uncertain assignment (multiple similar probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate entropy for each song\n",
    "def calculate_entropy(probs):\n",
    "    \"\"\"Calculate Shannon entropy of probability distribution\"\"\"\n",
    "    # Avoid log(0)\n",
    "    probs = np.clip(probs, 1e-10, 1)\n",
    "    return -np.sum(probs * np.log(probs), axis=1)\n",
    "\n",
    "entropies = calculate_entropy(probabilities)\n",
    "features_df['gmm_entropy'] = entropies\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"UNCERTAINTY ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nEntropy Statistics:\")\n",
    "print(f\"  Mean: {entropies.mean():.4f}\")\n",
    "print(f\"  Std: {entropies.std():.4f}\")\n",
    "print(f\"  Min: {entropies.min():.4f} (most confident)\")\n",
    "print(f\"  Max: {entropies.max():.4f} (most uncertain)\")\n",
    "\n",
    "# Find most/least confident assignments\n",
    "most_confident_idx = entropies.argmin()\n",
    "least_confident_idx = entropies.argmax()\n",
    "\n",
    "print(f\"\\nMost confident song:\")\n",
    "print(f\"  Index: {most_confident_idx}\")\n",
    "print(f\"  Entropy: {entropies[most_confident_idx]:.4f}\")\n",
    "print(f\"  Probabilities: {probabilities[most_confident_idx]}\")\n",
    "\n",
    "print(f\"\\nLeast confident song:\")\n",
    "print(f\"  Index: {least_confident_idx}\")\n",
    "print(f\"  Entropy: {entropies[least_confident_idx]:.4f}\")\n",
    "print(f\"  Probabilities: {probabilities[least_confident_idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize entropy distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Histogram\n",
    "ax1.hist(entropies, bins=50, edgecolor='black', alpha=0.7, color='skyblue')\n",
    "ax1.axvline(entropies.mean(), color='red', linestyle='--', linewidth=2, label='Mean')\n",
    "ax1.set_xlabel('Entropy', fontsize=12)\n",
    "ax1.set_ylabel('Frequency', fontsize=12)\n",
    "ax1.set_title('Distribution of Assignment Uncertainty', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Scatter: Max probability vs Entropy\n",
    "max_probs = probabilities.max(axis=1)\n",
    "ax2.scatter(max_probs, entropies, alpha=0.5, s=20)\n",
    "ax2.set_xlabel('Maximum Probability', fontsize=12)\n",
    "ax2.set_ylabel('Entropy', fontsize=12)\n",
    "ax2.set_title('Confidence vs Uncertainty', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Uncertainty analysis plots generated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Component Analysis\n",
    "\n",
    "### Means and Covariances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get component parameters\n",
    "means_scaled = gmm.means_\n",
    "covariances = gmm.covariances_\n",
    "weights = gmm.weights_\n",
    "\n",
    "# Transform means back to original scale\n",
    "means_original = scaler.inverse_transform(means_scaled)\n",
    "\n",
    "# Create DataFrame\n",
    "means_df = pd.DataFrame(\n",
    "    means_original,\n",
    "    columns=feature_cols,\n",
    "    index=[f'Component {i}' for i in range(10)]\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"GMM COMPONENT MEANS (ORIGINAL SCALE)\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "print(means_df)\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "sns.heatmap(means_df.T, \n",
    "            annot=True, \n",
    "            fmt='.2f',\n",
    "            cmap='RdYlGn',\n",
    "            center=means_df.values.mean(),\n",
    "            linewidths=1,\n",
    "            cbar_kws={\"label\": \"Feature Value\"},\n",
    "            ax=ax)\n",
    "\n",
    "ax.set_title('GMM Component Means - Feature Profiles', fontsize=14, fontweight='bold', pad=20)\n",
    "ax.set_xlabel('Component', fontsize=12)\n",
    "ax.set_ylabel('Feature', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mixing coefficients\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MIXING COEFFICIENTS (COMPONENT WEIGHTS)\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "weights_df = pd.DataFrame({\n",
    "    'Component': [f'Component {i}' for i in range(10)],\n",
    "    'Weight (Ï€)': weights,\n",
    "    'Percentage': weights * 100\n",
    "})\n",
    "\n",
    "print(weights_df.to_string(index=False))\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "ax.bar(range(10), weights, color='skyblue', edgecolor='black')\n",
    "ax.axhline(y=0.1, color='red', linestyle='--', linewidth=2, label='Equal weight (0.1)')\n",
    "ax.set_xlabel('Component', fontsize=12)\n",
    "ax.set_ylabel('Mixing Coefficient (Ï€)', fontsize=12)\n",
    "ax.set_title('GMM Mixing Coefficients', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(range(10))\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariance Structure\n",
    "\n",
    "Each component has its own covariance matrix $\\Sigma_k$:\n",
    "- **Diagonal elements** = variances\n",
    "- **Off-diagonal elements** = covariances between features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize covariance matrices for first 4 components\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "fig.suptitle('Covariance Matrices (First 4 Components)', fontsize=16, fontweight='bold')\n",
    "\n",
    "for idx in range(4):\n",
    "    row = idx // 2\n",
    "    col = idx % 2\n",
    "    ax = axes[row, col]\n",
    "    \n",
    "    # Get covariance matrix\n",
    "    cov_matrix = covariances[idx]\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(cov_matrix, \n",
    "                annot=True, \n",
    "                fmt='.3f',\n",
    "                cmap='coolwarm',\n",
    "                center=0,\n",
    "                square=True,\n",
    "                linewidths=1,\n",
    "                xticklabels=feature_cols,\n",
    "                yticklabels=feature_cols,\n",
    "                cbar_kws={\"shrink\": 0.8},\n",
    "                ax=ax)\n",
    "    \n",
    "    ax.set_title(f'Component {idx}', fontweight='bold', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Covariance matrices visualized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. GMM vs K-Means Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load K-Means results\n",
    "kmeans_labels = pd.read_csv('data/processed/kmeans_cluster_assignments.csv')['kmeans_cluster'].values\n",
    "\n",
    "# Compare hard assignments\n",
    "agreement = (kmeans_labels == hard_labels).mean()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"GMM vs K-MEANS COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nLabel Agreement: {agreement*100:.2f}%\")\n",
    "print(f\"Label Disagreement: {(1-agreement)*100:.2f}%\")\n",
    "\n",
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "conf_matrix = confusion_matrix(kmeans_labels, hard_labels)\n",
    "\n",
    "print(\"\\nConfusion Matrix (K-Means vs GMM):\")\n",
    "print(\"Rows = K-Means clusters, Columns = GMM components\\n\")\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "sns.heatmap(conf_matrix, \n",
    "            annot=True, \n",
    "            fmt='d',\n",
    "            cmap='Blues',\n",
    "            linewidths=0.5,\n",
    "            xticklabels=range(10),\n",
    "            yticklabels=range(10),\n",
    "            cbar_kws={\"label\": \"Number of Songs\"},\n",
    "            ax=ax)\n",
    "\n",
    "ax.set_title('K-Means vs GMM Cluster Mapping', fontsize=14, fontweight='bold', pad=20)\n",
    "ax.set_xlabel('GMM Component', fontsize=12)\n",
    "ax.set_ylabel('K-Means Cluster', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save GMM Model and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "joblib.dump(gmm, 'models/gmm_model.pkl')\n",
    "\n",
    "# Save cluster assignments\n",
    "features_df.to_csv('data/processed/gmm_cluster_assignments.csv', index=False)\n",
    "\n",
    "# Save component parameters\n",
    "means_df.to_csv('data/processed/gmm_means.csv')\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FILES SAVED\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nâœ… models/gmm_model.pkl\")\n",
    "print(\"âœ… data/processed/gmm_cluster_assignments.csv\")\n",
    "print(\"âœ… data/processed/gmm_means.csv\")\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary\n",
    "\n",
    "### GMM Results\n",
    "\n",
    "**Model Configuration:**\n",
    "- K = 10 components\n",
    "- Covariance type: Full\n",
    "- EM iterations: ~15-20 (converged)\n",
    "- Probabilistic (soft) assignments\n",
    "\n",
    "**Key Findings:**\n",
    "1. **Soft clustering** provides uncertainty estimates\n",
    "2. Some songs have **high confidence** (one dominant component)\n",
    "3. Other songs have **low confidence** (multiple similar components)\n",
    "4. GMM captures **elliptical cluster shapes** (vs K-Means spherical)\n",
    "5. Agreement with K-Means: ~70-80% (expected)\n",
    "\n",
    "**Advantages of GMM:**\n",
    "- Probabilistic framework\n",
    "- Uncertainty quantification\n",
    "- Flexible cluster shapes (elliptical)\n",
    "- Per-component covariance structure\n",
    "\n",
    "**Disadvantages:**\n",
    "- More parameters (slower training)\n",
    "- More complex interpretation\n",
    "- Potential overfitting with small data\n",
    "\n",
    "### Next Steps\n",
    "1. Apply PCA for visualization (Notebook 4)\n",
    "2. Detailed evaluation and comparison (Notebook 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
